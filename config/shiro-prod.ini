# Apache Shiro Configuration - Production Environment  
# HVDC Ontology Insight - 운영 환경용 (인증 필수)

[main]
# 운영 환경: 모든 접근에 인증 필요
# 세션 타임아웃: 8시간 (28800초)
sessionManager = org.apache.shiro.web.session.mgt.DefaultWebSessionManager
sessionManager.sessionIdCookie.httpOnly = true
sessionManager.sessionIdCookie.secure = true
sessionManager.globalSessionTimeout = 28800000

# 비밀번호 해싱 (bcrypt 권장)
passwordMatcher = org.apache.shiro.authc.credential.HashedCredentialsMatcher
passwordMatcher.hashAlgorithmName = bcrypt
passwordMatcher.hashIterations = 12

[users]
# 운영 계정 (실제 환경에서는 외부 인증 시스템 연동 권장)
# 비밀번호는 bcrypt 해시로 저장 (아래는 예시용 평문)
hvdc_admin = $2a$12$XYZ..., admin
hvdc_analyst = $2a$12$ABC..., analyst  
hvdc_readonly = $2a$12$DEF..., readonly
# 실제 해시 생성: python -c "import bcrypt; print(bcrypt.hashpw(b'password', bcrypt.gensalt()).decode())"

[roles]
# 역할 기반 권한 제어
admin = *
analyst = sparql:query, sparql:update, data:read, data:write, graph:OFCO, graph:DSV, graph:PKGS, graph:PAY
readonly = sparql:query, data:read
external = sparql:query

[urls]
# 헬스체크: 인증 없음 (모니터링용)
/$/ping = anon

# 관리 기능: admin만 접근
/$/stats = authc, roles[admin]
/$/server = authc, roles[admin] 
/$/datasets = authc, roles[admin]
/$/backup/** = authc, roles[admin]

# HVDC 데이터셋 접근 제어
/hvdc/sparql = authc, perms[sparql:query]
/hvdc/update = authc, perms[sparql:update]
/hvdc/data = authc, perms[data:write]

# Named Graph별 세분화 접근 제어 (고급)
/hvdc/data?graph=*OFCO* = authc, perms[graph:OFCO]
/hvdc/data?graph=*DSV* = authc, perms[graph:DSV]  
/hvdc/data?graph=*PKGS* = authc, perms[graph:PKGS]
/hvdc/data?graph=*PAY* = authc, perms[graph:PAY]

# 웹 UI (있는 경우)
/index.html = authc
/sparql.html = authc, perms[sparql:query]

# 기본: 모든 경로 인증 필요
/** = authc

## 운영 보안 체크리스트:
## ✅ HTTPS 필수 (nginx/Apache 프록시 권장)
## ✅ 방화벽: 3030 포트 내부 네트워크만 허용
## ✅ VPN/사내망 접근 제한
## ✅ 로그 모니터링 (접근 기록, 실패 시도)
## ✅ 정기 비밀번호 변경 (90일)
## ✅ 백업 데이터 암호화
## ✅ 감사 로그 보존 (1년)

## 고급 보안 옵션:
## - LDAP/Active Directory 연동
## - JWT 토큰 기반 인증  
## - IP 화이트리스트
## - Rate limiting
## - SQL Injection 방지 (SPARQL Injection)
